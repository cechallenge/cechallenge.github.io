---
title:
feature_text: |
feature_image: "/assets/logos/banner_white_with_slogan.png"
---

Large Language Model(LLM)은 Natural Language Processing(NLP), 번역, 각종 시험(SAT, AP exams, 의학 지식 등), coding challenge (LeetCode)등과 같은 다양한 분야에서 성과를 보여주고 있어서 수요가 폭발적으로 증가하고 있습니다. LLM을 많은 사람들이 이용하는 서비스로 활성화하기 위해서는 LLM에서 multiple GPU를 활용하여 inference의 latency를 줄이는 것이 매우 중요합니다. 하지만 아쉽게도 multiple GPU에서 LLM을 효율적으로 infernece하는 방법이 널리 알려져 있지는 않습니다.

삼성전자 SAIT는 LLM기반 서비스의 확산을 위하여 **Computer Engineering Challenge**를 개최하게 되었습니다. 이번 Challenge의 목표는 높은 정확도를 유지하면서 4개의 GPU를 활용하여 LLM의 latency를 절감하는 것입니다. 그리고 공정한 평가를 위해 latency 평가 방식과 데이터셋을 제시하고 각 submission에 대해 면밀한 분석을 진행 할 예정입니다. Computer Engineering Challenge에 참여한 팀 중에서 선정된 10개 팀은 2차 라운드에서 동일한 computing platform을 제공받아 팀에서 제시한 idea를 구현하고, latency를 비교하여 평가할 예정입니다.

이번 Computing Engineering Challenge를 통해 더 많은 학생들이 LLM을 가속하는 다양한 Computer Engineering 연구들에 흥미를 갖기를 희망합니다. 저희는 이러한 Computer Engineering 관심 증가가 앞으로 도래할 AI Computing 시대를 앞당기고 더불어 에너지와 비용을 절감하는데 크게 기여할 것으로 기대합니다.
<hr />
The Large Language Models (LLM) has shown significant performance in various fields such as Natural Language Processing (NLP), translation, various exams (SAT, AP exams, medical knowledge, etc.), and coding challenges (LeetCode), leading to explosive demand. To enable public services that many people use, it is important to reduce the latency of inference by utilizing multiple GPUs for LLM. However, there is not yet a widely known way to efficiently infer LLM on multiple GPUs.

Therefore, we are pleased to present the Computer Engineering Challenge to promote the spread of LLM-based services. The goal of the challenge is to reduce the latency of large-scale LLM using four GPUs while maintaining high accuracy. To ensure fair evaluation, we will provide latency evaluation method and dataset, and conduct extensive analysis for each submission. Among the submissions, 10 teams will be selected to compare the latency on the same computing platform for the second round we provide for fair performance evaluation.

Through this Computing Engineering Challenge, we hope that more students will become interested in various computer engineering research related to accelerating LLM. We expect this increase in computer engineering interest to make a significant contribution to advancing the coming era of AI computing and saving energy and costs.
