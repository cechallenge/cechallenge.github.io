---
title:
feature_text: |
feature_image: "/assets/logos/banner_white_with_slogan.png"
---

Large Language Model(LLM)은 Natural Language Processing(NLP), 번역, 각종 시험(SAT, AP exams, 의학 지식 등), coding challenge (LeetCode)등과 같은 다양한 분야에서 성과를 보여주고 있어서 수요가 폭발적으로 증가하고 있습니다. 많은 사람들이 이용하는 서비스를 활성화하기 위해서는 LLM에서 multiple GPU를 활용하여 inference의 latency를 줄이는 것이 매우 중요합니다. 하지만 아쉽게도 multiple GPU에서 LLM을 효율적으로 infernece하는 방법은 널리 알려져 있지는 않습니다.

삼성전자 SAIT는 LLM기반 서비스의 확산을 위해 Computer Engineering Challenge를 개최하게 되었습니다. 이번 Challenge의 목표는 높은 정확도를 유지하면서 4개의 GPU를 활용하여 대규모 LLM의 latency를 절감하는 것입니다. 그리고 공정한 평가를 위해 evaluation task와 data를 제공하고 각 submission에 대해 광범위한 분석을 진행 할 예정입니다. 제출된 팀 중에서 선정된 10팀에게는 동일한 computing platform을 제공하여 제시한 Idea를 구현하고 latency를 비교 평가할 예정입니다.

이번 Computing Engineering Challenge가 앞으로 도래할 AI Computing 시대를 앞당기고 에너지와 비용을 절감하는데 크게 기여할 것으로 기대합니다.

Large Language Models (LLM) has shown significant performance in various fields such as Natural Language Processing (NLP), translation, various exams (SAT, AP exams, medical knowledge, etc.), and coding challenges (LeetCode), leading to explosive demand. To enable public services that many people use, it is important to reduce the latency of inference by utilizing multiple GPUs for LLM. However, there is not yet a widely known way to efficiently infer LLM on multiple GPUs.

Therefore, we are pleased to present the Computer Engineering Challenge to promote the spread of LLM-based services. The goal of this challenge is to reduce the latency of large-scale LLM using four GPUs while maintaining high accuracy. To ensure fair evaluation, we will provide evaluation tasks and data, and conduct extensive analysis for each submission. Among the submissions, 10 teams will be selected to compare the latency on the same computing platform we provide for fair performance evaluation.

We expect that this challenge will contribute significantly to advancing the AI computing era that lies ahead and to saving engergy and costs.