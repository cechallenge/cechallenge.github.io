---
title:
feature_text: |
  ## 2023 Samsung Computing Engineering Challenge
  ### No System, No AI : Let's Play with LLM
feature_image: "https://picsum.photos/1300/400?image=989"
---

수많은 말뭉치로 학습된 대규모 언어 모델(Large Language Models)은 적은 양의 supervised 예제만으로 여러 가지 작업을 해결할 수 있는 능력을 가지고 있으며, 몇 년 동안 큰 주목을 받고 있습니다. 이러한 few-shot 학습된 모델은 ...

Large Language Models (LLMs) trained on large corpora of texts have attracted significant attention in recent years with their ability to solve tasks with few supervised examples. These few-shot models have shown state-of-the-art success across NLP tasks (Entity Recognition), language translation, standardized exams (SAT, AP exams, Medical Knowledge), coding challenges (LeetCode), as well as in subjective domains such as chatbots. All of these domains involve bootstrapping a single LLM referred to as a foundation model with examples of specific knowledge from the associated task. The process of updating a model with limited domain-specific data is known as fine-tuning. However, the costs of accessing, fine-tuning and querying foundation models to perform new tasks are large. Given these costs, access to performant LLMs has been gated behind expensive and often proprietary hardware used to train models, making them inaccessible to those without substantial resources.


Our goal is to democratize access to language models and address three major issues:

 1. TODO  
 2. TODO

Here we present a LLM efficiency challenge, to tackle these three challenges and democratize access to state of the art LLMs. Specifically, we introduce a challenge for the community to adapt a foundation model to specific tasks by fine-tuning on a TODO...